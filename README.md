# 📚 8주 NLP 논문 스터디 🚀

인공지능대학원 진학을 목표로 하면서 학술 연구에 필요한 핵심 역량을 미리 기르고자 시작된 스터디입니다. 특히 자연어처리 분야의 주요 논문들을 체계적으로 학습하여 논문 읽기, 분석, 발표 능력을 향상시키는 것을 목표로 합니다.

## 🎯 학습 목표

- **학술 논문의 주요 논점을 분석하고 종합하여 명확하고 체계적인 발표로 구현하는 의사소통 역량을 개발한다.**
- **논문을 기반으로 한 학습을 통해 학술적 표현과 전문 용어에 대한 이해도를 높인다.**
- **인용과 참고문헌 작성 등 학술적 글쓰기의 윤리적 기준과 관례를 숙지하고 실천한다.**

대학원 진학 후 연구 활동에서 필수적인 위와 같은 학술적 역량을 개발하고자 합니다.

## 📖 스터디 진행 방식

매주 **개인별 논문 읽기 → 자료 정리 → 발표 → 토론**의 과정으로 진행됩니다.

자연어처리(NLP)에 대한 기초 개념부터 현대적 모델인 Transformer까지, 주요 논문을 중심으로 학습하는 8주차 스터디입니다.  
단어 임베딩, 순환 신경망, 어텐션, 전이 학습, 트랜스포머 등 핵심 기술을 논문을 통해 체계적으로 이해합니다.

## 📄 Papers

| 주차  | 논문 제목                                                                    | 핵심 주제                        | 링크                                            |
| ----- | ---------------------------------------------------------------------------- | -------------------------------- | ----------------------------------------------- |
| 0주차 | NLP Tutorial                                                                 | Word2Vec 전 기초 지식 정리       | -                                               |
| 1주차 | Efficient Estimation of Word Representations in Vector Space (2013)          | Word2Vec (CBOW, Skip-gram)       | [논문 보기](https://arxiv.org/abs/1301.3781)    |
| 2주차 | GloVe: Global Vectors for Word Representation (2014)                         | GloVe 임베딩                     | [논문 보기](https://aclanthology.org/D14-1162/) |
| 3주차 | Sequence to Sequence Learning with Neural Networks (2014)                    | Seq2Seq (RNN 기반 인코더-디코더) | [논문 보기](https://arxiv.org/abs/1409.3215)    |
| 4주차 | Neural Machine Translation by Jointly Learning to Align and Translate (2014) | Bahdanau Attention               | [논문 보기](https://arxiv.org/abs/1409.0473)    |
| 5주차 | Deep Contextualized Word Representations (2018)                              | ELMo (문맥 기반 임베딩)          | [논문 보기](https://arxiv.org/abs/1802.05365)   |
| 6주차 | Universal Language Model Fine-tuning for Text Classification (2018)          | ULMFiT (전이 학습의 시작)        | [논문 보기](https://arxiv.org/abs/1801.06146)   |
| 7주차 | Attention Is All You Need (2017)                                             | Transformer 구조                 | [논문 보기](https://arxiv.org/abs/1706.03762)   |

## 🧭 스터디 팁

- **발표 자료 준비**: 슬라이드나 문서로 논문의 핵심 내용을 체계적으로 정리
- **질문 준비**: 논문에서 이해가 어려웠던 부분이나 궁금한 점을 미리 정리
- **코드 실습 (선택)**: 논문의 핵심 아이디어를 파이썬으로 구현하거나 관련 오픈소스를 분석
- **개념 정리**: 새로 학습한 용어나 구조를 문서화
- **기록 유지**: 노션 또는 구글독스를 통해 학습 기록, 토론 내용 정리

### ✅ 주간 진행 과정

1. **개인 학습**: 각자 해당 주차 논문을 읽고 핵심 내용을 파악
2. **자료 제작**: 논문의 주요 개념, 모델 구조, 수식, 실험 결과 등을 정리한 발표 자료 준비
3. **발표**: 정리한 내용을 바탕으로 논문의 핵심을 명확하게 설명
4. **토론**: 논문에서 궁금했던 부분, 이해가 어려운 개념, 관련 질문 등을 함께 논의

### ✅ 기대 효과

- **핵심 논문을 통해 NLP 주요 기술의 발전 흐름을 파악**
- **각 논문의 개념, 모델 구조, 수식, 실험 결과 등을 정리하는 능력 향상**
- **발표 및 토론을 통해 깊이 있는 이해 도모**

## 👥 Contributors

- [hyunohJang](https://github.com/hyunohJang)
- [D0won](https://github.com/D0won)
