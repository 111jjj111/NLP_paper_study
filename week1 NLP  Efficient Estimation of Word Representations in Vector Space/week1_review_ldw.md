# 개요

nlp 분야에서 대표격인 word2vec 논문을 읽어보고 이에 대한 강의를 들어보았다.
이 논문은 기존의 제시되었던 모델의 한계점을 명시하고 자신들이 만든 기존 모델보다 뛰어난 새로운 모델 2가지를 제시하였다.

# 논문을 보기 전

기존에 단어를 표현하는 건 One-hot으로 단어를 표현을 하였다. 다만 이는
희소행렬로 이루어져있으며 단어가 많으면 많을수록 공간을 매우 많이 차지하고 유사한 단어도 알 수 없다. 따라서 Embedding Layer를 통해서 one-hot-vector에서 dense vector로 변환해준다.

이는 기존 표현에서 특징을 추출하여 실수로 표현을 한다. 이 과정은 cv과정에서 이미지로부터 특징을 추출하기 위해 사용하는 Convolution layer와 비슷하다.

# 기존 모델들

논문에 제시된 기존 모델들은 두 가지가 있는데 한 가지는 NNLM(Feedforward Neural Net Language Model) 그리고 나머지 한 가지는 RNNLM(Recurrent Neural Net Language Model)이다.

## NNLM(Feedforward Neural Net Language Model)

NNLM은 앞의 N개의 단어 즉 문맥으로부터 다음 단어를 예측하는 모델로 이는 input, projection, hidden, output layer로 구성되어있다. 입력 계층에서 N개의 이전 단어가 어휘크기에 따라 one-hot-encoding이 되고 projection 행렬을 사용하여 N x D 차원을 갖는
projection 계층 P로 투영된다. 여기서 D는 각 단어의 임베딩 차원을 나타낸다.

이렇게 투영된 단어 벡터들은 일반적으로 합쳐지거나 평균을 내어 하나의 벡터를 형성한 뒤 비선형 활성화 함수를 사용하는 은닉 계층을 거쳐 벡터를 생성한다. 생성된 벡터는 문맥 정보를 압축한 표현이라고 볼 수 있다.
마지막으로, 은닉 계층에서 나온 벡터는 출력 계층으로 전달되어 다음 단어로 올 수 있는 각 단어에 대한 확률 분포를 예측한다.

NNLM은 은닉층 연산으로 인해 계산 비용이 크고 앞의 N개의 단어로 문맥이 고정되어 있어 긴 의존성을 반영하기 어렵다는 한계가 있다.

## RNNLM(Recurrent Neural Net Language Model)

RNNLM은 순환신경망을 이용해 긴 문맥을 반영하는 모델로 projection layer가 없고 input, hidden, output layer만 있다. 이 모델의 특징은 이전 시점의 정보가 다음 시점으로 전달된다. 이를 시간 지연 연결이라 하는데 이를 사용하여 hidden layer 자체에 연결하는 가중치 행렬 recurrent 행렬이 있다. 이를 통해 recurrent 모델은 일종의 단기 기억을 형성한다. 각 시점에서 hidden layer는 현재 입력 단어와 바로 직전 시점의 hidden layer를 조합하여 업데이트하기 때문이다.

RNNLM은 시퀀스가 길어질수록 과거 정보를 기억하기 어렵고 기울기 소실/폭주 문제(모델이 올바른 방향으로 학습하기 힘듬)에 취약하다. 본질적으로 순차적으로 처리해야하기 때문에 병렬 학습이 어려워 학습 시간이 오래 걸린다.

# 제시된 새로운 모델들

이 논문에서 계산 복잡도를 최소화하기 위해 두 가지 새로운 분산 표현 학습 모델 구조를 제안했다.
이전 섹션에서는 non-linear hidden layer가 대부분의 복잡성을 유발한다는 점에서 훨씬 효율적으로 대량의 데이터를 학습할 수 있는 더 간단한 모델을 제시하였다.

## CBOW(Continuous Bag-of-Words Model)

이 모델은 주변 단어를 보고 중심 단어를 예측하는 방식이다. 이 때 분포 가설에 기반을 두는데 분포 가설이란 비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다는 뜻이다.
쉽게 말해 인간관계로 친다면 유유상종이라는 뜻이다.

CBOW 모델의 구조는 input layer, projection layer, output layer로 구성된다.
input layer에서는 N개의 주변 단어들이 각각 one-hot vector 형태로 입력된다.
projection layer에서는 입력된 각 one-hot vector는 VxM 형태의 가중치 행렬을 통해 M차원의 임베딩 벡터로 투영된다. 여기서 V는 어휘 사전의 크기, M은 임베딩 벡터의 차원이다. 이 때 주변 단어들의 임베딩 벡터들은 합쳐지거나 평균을 내어 하나의 벡터로 통합된다.
output layer에서는 앞선 계층에서 생성된 벡터는 다시 MxV 형태의 가중치 행렬을 통해 V만큼의 점수 벡터로 반환된다. 이 점수 벡터는 소프트맥스 함수를 적용하여 각 단어가 중심 단어가 될 확률을 예측한다.

## Skip-gram (reverse CBOW)

Skip-gram은 CBOW와 반대로 중심 단어를 보고 주변 단어들을 예측하는 방식이다. 이는 희귀단어에 대한 예측에서 사용되는데 희귀단어는 CBOW 모델에서 예측이 매우 안되는 경향이 있다. 따라서 이러한 희귀단어의 주변을 보고 경향성을 파악하는게 이러한 학습 과정의 목표라고 볼 수 있다.

Skip-gram의 구조는 CBOW와 같다.
input layer에서 하나의 중심단어가 one-hot vector 형태로 입력된다.
projection layer에서 입력된 중심 단어의 one-hot vector는 가중치 행렬을 통해 M차원의 임베딩 벡터로 투영된다. CBOW와 달리 여기서는 하나의 단어만 입력되므로 이 계층을 거치면 해당 중심 단어의 임베딩 벡터가 바로 추출된다.
Output layer에서 projection layer에서 생성된 중심 단어의 임베딩 벡터를 바탕으로, 여러 개의 주변 단어를 각각 독립적으로 예측한다. 각 주변 단어에 대해 V만큼의 점수 벡터를 출력하고, 여기에 소프트맥스 함수를 적용하여 각 단어가 주변 단어가 될 확률을 예측한다.

# 결론
본 논문 리뷰를 통해 NLP 분야의 대표적인 단어 임베딩 모델인 Word2Vec의 핵샘 개념과 그 배경을 살펴보았다. 기존의 One-hot-encoding 방식이 가진 문제점을 인지하고 초기 신경망 모델인 NNLM의 한계점을 파악했다.

Word2Vec은 계산 복잡도를 최소화하기 위한 두 가지 혁신적인 로그-선형 모델, 즉 CBOW와 Skip-gram을 제시했다. CBOW는 주변 단어로부터 중심 단어를 예측하고 Skip-gram은 반대로 중심 단어를 통해 주변 단어들을 예측하는 방식을 취하는 것을 알 수 있었다.

결과적으로 Word2Vec은 효율적이면서도 의미론적, 문법적 관계를 잘 반영하는 단어 임베딩을 가능하게 하여, 현대 자연어 처리의 다양한 분야에서 핵심적인 기술로 자리매김을 하였다.
