# 개요

오늘은 NLP 분야 유명한 논문인 Sequence to Sequence Learning with Neural Networks에 대하여 논문과 이에 관련한 강의를 들었다.
Sequence to Sequence 논문에서는 신경망을 이용하여 입력 시퀀스를 출력 시퀀스로 매핑하는 일반적인 방법을 제안한다. 이 모델은 인코더-디코더 구조를 사용하여 기계 번역, 질의응답 등 다양한 sequence-to-sequence 문제를 해결할 수 있는 범용적인 프레임워크를 제시했다. 특히 LSTM을 활용하여 긴 시퀀스에서도 효과적으로 학습할 수 있음을 보여주었으며 이는 현대 자연어 처리의 기반이 되는 중요한 연구로 평가받고 있다.

# 논문을 보기 전

본격적인 논문 내용을 살펴보기 전에 논문에서 자주 언급되는 Sequence data와 LSTM 개념을 간단히 정리하겠다.

## Sequence data
Sequence data란 순서대로 정렬된 데이터의 연속이다. 예를 들어 순서가 있는 문자열, 시간에 따라 달라지는 주식 가격과 같은 시계열 데이터 등이 있다.
Sequence data는 RNN과 적합하며 DNN과는 적합하지 않다. 이는 RNN이 순차적인 데이터에서 이전 정보들을 활용하여 학습할 수 있는 구조를 가지고 있기 때문이며 DNN은 독립적인 입력 데이터에 더 적합하기 때문이다.

## LSTM
LSTM 이란 Long-Short-Term-Memory의 약자로 일반 RNN의 문제들을 보완한 RNN 계열 Neural-net이다. LSTM은 우리 사람의 뇌와 같이 중요한 정보는 오래 기억하고 불필요한 정보는 잊어버리도록 설계되어있다.
LSTM은 이전 hidden state와 input에서 들어온 새로운 정보는 각각 forget gate, input gate, output gate를 통해 처리되며, 이들의 결과는 cell state를 업데이트하여 장기 기억을 유지하거나 수정한다.
작동과정을 살펴보면,
1. Input gate
    - 현재 입력(Input)과 이전 hidden state에서 받은 정보가 결합되어 시그모이드 활성화 함수를 통과한다. 이 결과는 0~1 사이의 값으로 새로운 정보를 얼마나 cell state에 추가할 지를 결정한다.
2. Forget gate
    - 이전 hidden state에서 받은 정보와 현재 입력 받은 정보가 시그모이드 활성화 함수를 통과하여 0~1의 값을 생성하고 이 값은 이전 cell state에서 어떤 정보를 잊을지 결정한다.
3. Cell state 업데이트
    - forget gate의 출력과 이전 cell state가 곱해져 잊어야 할 정보를 제거한다. input gate의 출력과 새로운 정보가 곱해져 cell state에 추가된다.
4. Output Gate
    - 현재 입력과 이전 hidden state가 결합되어 시그모이드 활성화 함수를 통과한다. 이 값은 최종적으로 출력될 정보를 결정한다. 업데이트된 cell state는 tanh 활성화 함수를 통과하여 output gate의 결과와 곱해져 최종 hidden state를 생성한다.

# The model
앞서 설명한 LSTM의 장점을 활용하여 Sequence to Sequence 논문에서는 인코더-디코더 구조를 제안한다. 인코더 부분에서는 LSTM을 사용하여 입력 시퀀스를 순차적으로 처리한다. LSTM의 메커니즘을 통해 중요한 정보는 기억하고 불필요한 정보는 잊어가면서 전체 입력 정보를 하나의 고정 크기 벡터(context vector)로 압축한다. 이때 논문에서는 성능 향상을 위해 입력 시퀀스를 역순으로 넣는 기법을 사용한다.
디코더 부분에서도 LSTM을 사용하며 인코더에서 생성된 context vector를 초기 상태로 받아 출력 시퀀스를 생성한다. 디코더는 자기회귀(Autoregressive) 방식으로 작동하는데, 이는 이전에 생성된 출력을 다음 시점의 입력으로 사용하여 순차적으로 토큰을 생성하는 방식이다. 이 과정에서도 LSTM의 cell state와 hidden state를 통해 이전 문맥 정보를 유지하면서 다음 토큰을 예측할 수 있다.
이 모델의 핵심은 조건부 확률을 이용한 시퀀스 생성이다. 입력 시퀀스가 주어진 조건에서 전체 출력 시퀀스가 생성될 확률은 체인 룰에 의해 각 단계별 조건부 확률들의 곱으로 분해된다. 즉, 각 시점에서 이전까지 생성된 모든 토큰들을 조건으로 하여 다음 토큰이 생성될 확률을 계산하고, 이들을 모두 곱한 것이 전체 시퀀스의 확률이 된다. 예를 들어 기계 번역에서 "나는 학생이다"를 "I am a student"로 번역할 때, 먼저 "I"가 생성되고, 다음에는 "I"가 주어진 조건에서 "am"이 생성되며, 그 다음에는 "I am"이 주어진 조건에서 "a"가 생성되는 방식으로 순차적으로 진행된다.

# 기존 SMT 모델과 현재 모델
기존 SMT(Statistical Machine Translation) 모델은 통계 기반 확률 모델로 구조가 굉장히 복잡하다. 단위는 단어, 구문 단위 조합으로 출력 문장이 종종 어색하지만 학습 데이터가 적고 그 대신 이를 해석할 수 있는 장점이 있다. 이 논문에서는 기존 SMT 모델과 자신들이 제시한 모델의 실험 결과를 비교하고 있는데 데이터는 WMT'14 English to French이며 이름에서 알 수 있듯 영어를 프랑스어로 번역하기 위한 데이터이다. 평가 지표로는 BLEU 점수를 사용했으며, 점수가 높을수록 번역 품질이 좋다는 의미이다. 실험 결과를 살펴보면, 기존 baseline 시스템은 33.30점을 기록했다. 반면 논문에서 제시한 단일 LSTM 모델은 beam search를 적용했을 때 30.59점으로 기존 모델보다 낮은 성능을 보였다. 하지만 여러 LSTM 모델을 앙상블로 구성했을 때는 상황이 달라졌다. 5개의 역순 LSTM을 앙상블로 사용하고 beam size를 2로 설정했을 때 34.50점을 기록하여 기존 SMT 모델을 넘어섰다. 특히 beam size를 12로 늘렸을 때는 34.81점으로 가장 높은 성능을 달성했다. 이는 단일 모델로는 기존 SMT 모델에 미치지 못하지만, 여러 모델을 조합하고 적절한 디코딩 전략을 사용하면 기존 모델을 능가할 수 있음을 보여준다. 또한 입력 시퀀스를 역순으로 처리하는 것이 성능 향상에 중요한 역할을 했음을 알 수 있다.

# 결론
이 논문을 통해 연구진들은 제한된 어휘를 가진 대규모 깊은 LSTM이 문제 구조에 대한 거의 아무런 가정 없이도 표준 SMT 기반 시스템을 능가할 수 있음을 보여주었다. 이는 간단한 LSTM 기반 접근법이 기계 번역뿐만 아니라 충분한 훈련 데이터가 있는 다른 시퀀스 학습 문제에서도 잘 작동할 것임을 시사한다. 특히 연구진들은 소스 문장의 단어 순서를 뒤바꾸는 것만으로도 얻을 수 있는 성능 향상의 정도에 놀랐다고 밝혔다. 이는 단기 의존성의 수를 최대한 많이 가지는 문제 인코딩을 찾는 것이 중요하며, 이를 통해 학습 문제를 훨씬 간단하게 만들 수 있음을 의미한다. 비록 역순이 아닌 번역 문제에서는 표준 RNN을 훈련시킬 수 없었지만, 소스 문장을 역순으로 처리할 때는 표준 RNN도 쉽게 훈련 가능할 것이라고 판단했다. 또한 LSTM이 매우 긴 문장을 올바르게 번역할 수 있는 능력도 놀라운 발견이었다. 연구진들은 처음에 제한된 메모리로 인해 LSTM이 긴 문장에서 실패할 것이라고 확신했지만, 역순 데이터셋으로 훈련된 LSTM은 긴 문장 번역에서도 거의 어려움을 겪지 않았다. 가장 중요한 것은 간단하고 직관적이며 상대적으로 최적화되지 않은 접근법이 SMT 시스템을 능가할 수 있다는 점이다. 이러한 결과는 향후 연구를 통해 더욱 향상된 번역 정확도를 달성할 수 있을 것이며, 이 접근법이 다른 도전적인 시퀀스 간 문제에서도 잘 작동할 가능성을 보여준다.



+) 코파일럿의 도움을 일부 받았습니다.(표현, 문장 흐름 등)

